{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas Utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import argparse\n",
    "import logging\n",
    "import logging.config\n",
    "import sys\n",
    "from pathlib import Path\n",
    "#from utils.json import get_json_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def read_coco_json(json_path):\n",
    "    \"\"\"\n",
    "    Reads a COCO format JSON file and returns the data.\n",
    "    \n",
    "    Args:\n",
    "    - json_path (str or Path): Path to the JSON file.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Parsed JSON data.\n",
    "    \"\"\"\n",
    "    json_path = Path(json_path)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not json_path.exists():\n",
    "        logger.error(f\"File not found: {json_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Loading COCO annotations...\")\n",
    "        with open(json_path, 'r') as f:\n",
    "            coco_data = json.load(f)\n",
    "            logger.info(\"Successfully loaded the JSON file.\")\n",
    "            return coco_data\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {json_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(f\"Invalid JSON in file: {json_path}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Path to the COCO annotations JSON file\n",
    "    json_path = \"/\"\n",
    "    \n",
    "    # Read the JSON file\n",
    "    coco_data = read_coco_json(json_path)\n",
    "    \n",
    "    if coco_data:\n",
    "        # Access some basic information\n",
    "        images = coco_data.get('images', [])\n",
    "        annotations = coco_data.get('annotations', [])\n",
    "        categories = coco_data.get('categories', [])\n",
    "        \n",
    "        logger.info(f\"Number of images: {len(images)}\")\n",
    "        logger.info(f\"Number of annotations: {len(annotations)}\")\n",
    "        logger.info(f\"Number of categories: {len(categories)}\")\n",
    "        \n",
    "        # Print details of the first few images\n",
    "        for image in images[:5]:\n",
    "            logger.info(f\"Image ID: {image['id']}, File Name: {image['file_name']}\")\n",
    "        \n",
    "        # Print details of the first few annotations\n",
    "        for annotation in annotations[:5]:\n",
    "            logger.info(f\"Annotation ID: {annotation['id']}, Image ID: {annotation['image_id']}, Category ID: {annotation['category_id']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def copy_images(images, src_dir, dest_dir):\n",
    "    \"\"\"\n",
    "    Copy selected images to a specified directory.\n",
    "    \"\"\"\n",
    "    for image in images:\n",
    "        try:\n",
    "            src_path = Path(src_dir) / image['file_name']\n",
    "            dest_path = Path(dest_dir) / src_path.name\n",
    "            shutil.copy(src_path, dest_path)\n",
    "            logger.info(f\"Successfully copied {src_path} to {dest_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to copy {src_path} to {dest_path}: {e}\")\n",
    "\n",
    "def split_dataset(images_dir, labels_json_path, output_dir, train_ratio=0.75, val_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits a COCO dataset into training, validation, and testing sets based on given ratios.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Loading COCO annotations...\")\n",
    "        with open(labels_json_path, 'r') as f:\n",
    "            coco_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {labels_json_path}\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(f\"Invalid JSON in file: {labels_json_path}\")\n",
    "        return\n",
    "\n",
    "    images_dir = Path(images_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "\n",
    "    if not images_dir.exists():\n",
    "        logger.error(f\"Images directory does not exist: {images_dir}\")\n",
    "        return\n",
    "\n",
    "    # Extract image and annotation details\n",
    "    images = coco_data.get('images', [])\n",
    "    annotations = coco_data.get('annotations', [])\n",
    "\n",
    "    # Validate ratios\n",
    "    if not (0 < train_ratio < 1 and 0 <= val_ratio < 1 and train_ratio + val_ratio <= 1):\n",
    "        logger.error(\"Invalid training/validation ratios.\")\n",
    "        return\n",
    "\n",
    "    random.shuffle(images)\n",
    "    total_images = len(images)\n",
    "    train_end = int(total_images * train_ratio)\n",
    "    val_end = train_end + int(total_images * val_ratio)\n",
    "\n",
    "    train_images = images[:train_end]\n",
    "    val_images = images[train_end:val_end]\n",
    "    test_images = images[val_end:]\n",
    "\n",
    "    def filter_annotations(images_set):\n",
    "        image_ids = {image['id'] for image in images_set}\n",
    "        return [annotation for annotation in annotations if annotation['image_id'] in image_ids]\n",
    "\n",
    "    def create_coco_subset(images, annotations):\n",
    "        return {\n",
    "            'images': images,\n",
    "            'annotations': annotations,\n",
    "            'categories': coco_data['categories']\n",
    "        }\n",
    "\n",
    "    for type, images_set in zip([\"train\", \"val\", \"test\"], [train_images, val_images, test_images]):\n",
    "        try:\n",
    "            images_output_path = output_dir / \"images\" / type\n",
    "            images_output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            labels_output_path = output_dir / \"labels\" / type\n",
    "            labels_output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            copy_images(images_set, images_dir, images_output_path)\n",
    "\n",
    "            coco_file = create_coco_subset(images_set, filter_annotations(images_set))\n",
    "            with open(labels_output_path / \"coco.json\", 'w') as file:\n",
    "                json.dump(coco_file, file, indent=4)\n",
    "            logger.info(f\"Dataset for {type} saved successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process data for {type}: {e}\")\n",
    "\n",
    "# Set arguments for running in Jupyter\n",
    "images_dir = \"/home/thales/Documents/object_detection_workshop/Códigos Originais e Dados/Dataset/images000000000000\"\n",
    "coco_json_path = \"/home/thales/Documents/object_detection_workshop/Códigos Originais e Dados/Dataset/annotations/instances_default.json\" # Updated to point directly to the JSON file\n",
    "output_dir = \"/home/thales/Documents/object_detection_workshop/Saida\"\n",
    "train_ratio = 0.75\n",
    "val_ratio = 0.1\n",
    "\n",
    "# Call the function\n",
    "split_dataset(images_dir, coco_json_path, output_dir, train_ratio, val_ratio)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
